import torch

# Monkey patch for systems without xpu support to avoid AttributeError
if not hasattr(torch, "xpu"):
    class DummyXPU:
        def is_available(self):
            return False
        def empty_cache(self):
            pass
        def device_count(self):
            return 0
        def current_device(self):
            return "cpu"
        def synchronize(self):
            pass
        def __getattr__(self, name):
            # Fallback for any other attribute to prevent crash
            def method(*args, **kwargs):
                return None
            return method
    torch.xpu = DummyXPU()

import torch.distributed
if not hasattr(torch.distributed, "device_mesh"):
    class DummyDeviceMeshModule:
        class DeviceMesh:
            def __init__(self, *args, **kwargs):
                pass
    torch.distributed.device_mesh = DummyDeviceMeshModule()

import os

import scipy.io.wavfile as wavfile
import numpy as np
from datetime import datetime

# Stable Audio Open 1.0 ã¯ diffusers ã‚’ä½¿ç”¨ã—ã¾ã™
# Note: ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯GatedãªãŸã‚ã€é…å¸ƒæ™‚ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«HFãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›ã—ã¦ã‚‚ã‚‰ã†å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

def generate_bgm(vibe="ç©ã‚„ã‹", duration_seconds=30, output_dir="bgm", token=None):
    """
    Stable Audio Open 1.0 ã‚’ä½¿ç”¨ã—ã¦BGMã‚’ç”Ÿæˆã—ã€WAVã¨ã—ã¦ä¿å­˜ã™ã‚‹ã€‚
    """
    from diffusers import StableAudioPipeline
    
    import random
    
    # Vibeã‚’è‹±èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒªã‚¹ãƒˆã«ãƒãƒƒãƒ”ãƒ³ã‚° (ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åˆ·æ–°)
    vibe_prompts_map = {
        "ç©ã‚„ã‹": [
            "Soft solo felt piano. Slow tempo, minimalist, gentle touch. Lullaby, warm, peaceful, relaxing, sleeping baby, intimate room sound. 60bpm. [Loopable]",
            "Gentle solo accordion. Slow breathing pads, long sustain notes, soft melody. Warm, nostalgic, peaceful, relaxing atmosphere. 65bpm. [Loopable]",
            "Slow duet of piano and accordion. Gentle waltz time (3/4), soft interaction. Relaxing, soothing, warm family moment. 70bpm. [Loopable]"
        ],
        "ã‚¨ãƒãƒ«ã‚®ãƒƒã‚·ãƒ¥": [
            "Upbeat duet of accordion and piano. Marching rhythm, bright and sunny melody. Energetic, happy, optimistic, outdoor picnic vibe. 125bpm.",
            "Lively accordion-led melody with rhythmic piano backing. Fast folk dance, jig style. Energetic bellows, joyful, sunny, countryside vibe. 130bpm.",
            "Fast-paced solo piano with light accordion accents. Major key arpeggios, bright and sparkling. Energetic, running children, pure joy. 135bpm."
        ],
        "æ„Ÿå‹•çš„": [
            "Cinematic emotional duet. Expressive piano arpeggios and nostalgic accordion melody. Builds to a crescendo. Touching, heart-warming, grand finale. 80bpm. [Non-looping]",
            "Nostalgic solo accordion waltz. French musette style, expressive bellows. Melancholic, beautiful, bittersweet memory. Ends with a slow fade. 75bpm. [Non-looping]",
            "Emotional solo grand piano. Simple but powerful melody. Expressive dynamics, reverb. Sentimental, touching, pure love. Ends with a long sustain chord. 70bpm. [Non-looping]"
        ],
        "ã‹ã‚ã„ã„": [
            "Bright and happy ukulele and glockenspiel. Wholesome, pastel color vibe. Sunny Sunday morning, cute pets, relaxing and acoustic. 100bpm.",
            "Playful solo acoustic guitar and light percussion. Innocent, heartwarming, kids playing in the park. Sweet, simple, and catchy melody. 110bpm.",
            "Bouncy piano and recorder melody. Lighthearted, cute, and optimistic. Educational video background, pure joy, smiling faces. 120bpm."
        ]
    }

    choices = vibe_prompts_map.get(vibe, vibe_prompts_map["ç©ã‚„ã‹"])
    prompt = random.choice(choices)
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä¸€éƒ¨ã‚’æŠœç²‹ã—ã¦ã‚¹ã‚¿ã‚¤ãƒ«åã¨ã—ã¦ä½¿ç”¨ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åç”¨ï¼‰
    style_keywords = ["piano", "guitar", "synth", "violin", "lofi", "8-bit", "orchestra", "ambient"]
    detected_style = "music"
    for kw in style_keywords:
        if kw in prompt.lower():
            detected_style = kw
            break

    # Vibeã«å¿œã˜ãŸãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å
    base_name = vibe
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{base_name}_{detected_style}_{timestamp}.wav"
    
    # Ensure absolute path if it looks relative
    if not os.path.isabs(output_dir):
        from utils import get_app_dir
        output_dir = os.path.join(get_app_dir(), output_dir)
        
    os.makedirs(output_dir, exist_ok=True)
        
    output_path = os.path.join(output_dir, filename)

    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, filename)

    # Stable Audio Open 1.0 ã¯æœ€å¤§47ç§’ã¾ã§å¯¾å¿œ
    # 47ç§’ã‚’è¶…ãˆã‚‹å‹•ç”»ï¼ˆä¾‹: 67ç§’ï¼‰ã®å ´åˆã€åŠåˆ†ã®é•·ã•ï¼ˆ33.5ç§’ï¼‰ã‚’2å›ãƒ«ãƒ¼ãƒ—ã•ã›ã‚‹æ–¹ãŒ
    # 1ã¤ã®é•·ã„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æœ«å°¾ã§ç„¡ç†ã‚„ã‚Šç¹‹ãã‚ˆã‚ŠéŸ³æ¥½çš„ã«è‡ªç„¶ã«ãªã‚Šã‚„ã™ã„ã€‚
    is_looped = False
    if duration_seconds > 47.0:
        audio_duration = duration_seconds / 2.0
        is_looped = True
    else:
        audio_duration = min(duration_seconds, 47.0)

    print(f"\n" + "="*50)
    print(f"ğŸ¬ AI BGM GENERATION: {vibe}")
    print(f"="*50)
    print(f"  - Style:    {detected_style}")
    if is_looped:
        print(f"  - Length:   {duration_seconds}s -> {audio_duration:.1f}s (Loop-optimized)")
    else:
        print(f"  - Length:   {audio_duration:.1f}s (Model Limit: 47s)")
    print(f"  - Output:   {os.path.basename(output_path)}")
    print(f"  - Prompt:   {prompt}")
    print(f"-"*50)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    # Mac M1/M2/M3 ã®å ´åˆã¯ mps ã‚’å„ªå…ˆ
    if torch.backends.mps.is_available():
        device = "mps"
    
    # mps ã¯ float16 ã¾ãŸã¯ float32
    dtype = torch.float16 if device != "cpu" else torch.float32
    
    try:
        print(f"  ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­... ({device}, {dtype})")
        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        # token ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
        pipe = StableAudioPipeline.from_pretrained(
            "stabilityai/stable-audio-open-1.0", 
            torch_dtype=dtype,
            token=token
        )
        pipe = pipe.to(device)
        
        # ç”Ÿæˆ
        print(f"  éŸ³æ¥½ã‚’ç”Ÿæˆä¸­...")
        # ç”Ÿæˆå®Ÿè¡Œ
        output = pipe(
            prompt, 
            num_inference_steps=50, 
            audio_end_in_s=audio_duration
        ).audios
        print() # Force newline after tqdm progress bar
        
        # output[0] ã¯ç¬¬ä¸€ç”Ÿæˆã‚µãƒ³ãƒ—ãƒ« (channels, samples)
        # scipy.io.wavfile.write ã®ãŸã‚ã« NumPy é…åˆ—ã«å¤‰æ›
        audio_data = output[0]
        if hasattr(audio_data, "cpu"):
            audio_data = audio_data.cpu().numpy()
        elif hasattr(audio_data, "numpy"):
            audio_data = audio_data.numpy()
        
        # 44.1kHz ã§ä¿å­˜
        # scipy.io.wavfile.write ã¯ float16 ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„ãŸã‚ float32 ã«å¤‰æ›
        # ã¾ãŸã€(samples, channels) ã‚’æœŸå¾…ã™ã‚‹ãŸã‚è»¢ç½®
        audio_data_t = audio_data.T.astype(np.float32)
        
        wavfile.write(output_path, 44100, audio_data_t)
        
        print(f">>> BGMç”Ÿæˆå®Œäº†: {output_path}")
        return True, output_path
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"Error during BGM generation: {e}")
        if "gated" in str(e).lower() or "not found" in str(e).lower() or "unauthorized" in str(e).lower():
            print("ãƒ’ãƒ³ãƒˆ: ã“ã®ãƒ¢ãƒ‡ãƒ«ã«ã¯ Hugging Face ãƒˆãƒ¼ã‚¯ãƒ³ãŒå¿…è¦ã§ã™ã€‚è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return False, None

if __name__ == "__main__":
    import sys
    test_token = os.getenv("HF_TOKEN")
    success, _ = generate_bgm(vibe="ç©ã‚„ã‹", duration_seconds=10, token=test_token)
    sys.exit(0 if success else 1)
